{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flashrank in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (0.2.10)\n",
      "Requirement already satisfied: tokenizers in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from flashrank) (0.21.1)\n",
      "Requirement already satisfied: onnxruntime in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from flashrank) (1.21.0)\n",
      "Requirement already satisfied: numpy in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from flashrank) (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from flashrank) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from flashrank) (4.67.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from onnxruntime->flashrank) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from onnxruntime->flashrank) (25.2.10)\n",
      "Requirement already satisfied: packaging in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from onnxruntime->flashrank) (24.2)\n",
      "Requirement already satisfied: protobuf in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from onnxruntime->flashrank) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from onnxruntime->flashrank) (1.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests->flashrank) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests->flashrank) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests->flashrank) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests->flashrank) (2025.1.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from tokenizers->flashrank) (0.29.3)\n",
      "Requirement already satisfied: colorama in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from tqdm->flashrank) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (4.12.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from coloredlogs->onnxruntime->flashrank) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from sympy->onnxruntime->flashrank) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime->flashrank) (3.5.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install flashrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to speedup LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = [\n",
    "   {\n",
    "      \"id\":1,\n",
    "      \"text\":\"Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.\",\n",
    "      \"meta\": {\"additional\": \"info1\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":2,\n",
    "      \"text\":\"LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper\",\n",
    "      \"meta\": {\"additional\": \"info2\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":3,\n",
    "      \"text\":\"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
    "      \"meta\": {\"additional\": \"info3\"}\n",
    "\n",
    "   },\n",
    "   {\n",
    "      \"id\":4,\n",
    "      \"text\":\"Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.\",\n",
    "      \"meta\": {\"additional\": \"info4\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":5,\n",
    "      \"text\":\"vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels\",\n",
    "      \"meta\": {\"additional\": \"info5\"}\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank.Ranker import Ranker, RerankRequest\n",
    "\n",
    "def get_result(query,passages,choice):\n",
    "  if choice == \"Nano\":\n",
    "    ranker = Ranker()\n",
    "  elif choice == \"Small\":\n",
    "    ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
    "  elif choice == \"Medium\":\n",
    "    ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
    "  elif choice == \"Large\":\n",
    "    ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
    "  rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "  results = ranker.rerank(rerankrequest)\n",
    "  print(results)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitesh\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 121 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Hitesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': np.float32(0.016847236)}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': np.float32(0.011547275)}, {'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': np.float32(0.00081340264)}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': np.float32(0.0006362861)}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': np.float32(0.00024851)}]\n",
      "CPU times: total: 531 ms\n",
      "Wall time: 398 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': np.float32(0.016847236)},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': np.float32(0.011547275)},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': np.float32(0.00081340264)},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': np.float32(0.0006362861)},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': np.float32(0.00024851)}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Nano\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': np.float32(0.94961107)}, {'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': np.float32(0.8347905)}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': np.float32(0.7039973)}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': np.float32(0.5282721)}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': np.float32(0.005023938)}]\n",
      "CPU times: total: 3.05 s\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': np.float32(0.94961107)},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': np.float32(0.8347905)},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': np.float32(0.7039973)},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': np.float32(0.5282721)},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': np.float32(0.005023938)}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (0.3.45)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (0.3.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (3.11.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (0.3.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_community) (2.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_google_genai in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: python_dotenv in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_google_genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_google_genai) (0.6.16)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.43 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_google_genai) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain_google_genai) (2.10.6)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.29.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.3.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\demo projects\\advanced rag - sunny\\advanced-rag-07-flashr-rank\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.43->langchain_google_genai) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_google_genai python_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyArxzKAiRsw6YaOr4hU8v0q6Yr1_5K-dJM'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "# Access environment variables\n",
    "os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "# Initialize with an embedding model\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "documents = TextLoader(\"stare_of_the_union.txt\", encoding=\"utf-8\").load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, text in enumerate(texts):\n",
    "    text.metadata[\"id\"] = id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='And we will, as one people. \n",
      "\n",
      "One America. \n",
      "\n",
      "The United States of America. \n",
      "\n",
      "May God bless you all. May God protect our troops.' metadata={'source': 'stare_of_the_union.txt', 'id': 95}\n"
     ]
    }
   ],
   "source": [
    "retriever = InMemoryVectorStore.from_documents(texts, embeddings).as_retriever(search_kwargs={\"k\": 10})\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 73}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \n",
      "\n",
      "While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 79}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "So let’s not abandon our streets. Or choose between safety and equal justice. \n",
      "\n",
      "Let’s come together to protect our communities, restore trust, and hold law enforcement accountable. \n",
      "\n",
      "That’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 68}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n",
      "\n",
      "I’ve worked on these issues a long time. \n",
      "\n",
      "I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety. \n",
      "\n",
      "So let’s not abandon our streets. Or choose between safety and equal justice.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 67}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "And I know you’re tired, frustrated, and exhausted. \n",
      "\n",
      "But I also know this. \n",
      "\n",
      "Because of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \n",
      "we are moving forward safely, back to more normal routines.  \n",
      "\n",
      "We’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \n",
      "\n",
      "Just a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 55}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "And tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \n",
      "\n",
      "By the end of this year, the deficit will be down to less than half what it was before I took office.  \n",
      "\n",
      "The only president ever to cut the deficit by more than one trillion dollars in a single year. \n",
      "\n",
      "Lowering your costs also means demanding more competition. \n",
      "\n",
      "I’m a capitalist, but capitalism without competition isn’t capitalism. \n",
      "\n",
      "It’s exploitation—and it drives up prices.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 50}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "I ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \n",
      "\n",
      "And I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home—they have no serial numbers and can’t be traced. \n",
      "\n",
      "And I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \n",
      "\n",
      "Ban assault weapons and high-capacity magazines.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 70}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "Get rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \n",
      "\n",
      "If you’re suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. \n",
      "\n",
      "Second, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 81}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "And we will, as one people. \n",
      "\n",
      "One America. \n",
      "\n",
      "The United States of America. \n",
      "\n",
      "May God bless you all. May God protect our troops.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 95}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \n",
      "\n",
      "And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.\n",
      "Metadata: {'source': 'stare_of_the_union.txt', 'id': 14}\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(query)\n",
    "pretty_print_docs(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c417ed6a-4b2d-4e40-8c6c-933d1b814f9d-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-ranking the output. \n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MultiBERT-L-12...\n",
      "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:44<00:00, 2.33MiB/s]\n"
     ]
    }
   ],
   "source": [
    "compressor = FlashrankRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "# It will give you the answer in compress format or contential one. \n",
    "compressed_docs = compression_retriever.invoke(\"What did the president say about Ketanji Jackson Brown\")\n",
    "len(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 73, 'relevance_score': np.float32(0.9987557), 'source': 'stare_of_the_union.txt'}, page_content='One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.'),\n",
       " Document(metadata={'id': 55, 'relevance_score': np.float32(0.9898337), 'source': 'stare_of_the_union.txt'}, page_content='And I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving forward safely, back to more normal routines.  \\n\\nWe’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \\n\\nJust a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines.'),\n",
       " Document(metadata={'id': 50, 'relevance_score': np.float32(0.9866672), 'source': 'stare_of_the_union.txt'}, page_content='And tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism. \\n\\nIt’s exploitation—and it drives up prices.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 55, 50]\n"
     ]
    }
   ],
   "source": [
    "print([doc.metadata[\"id\"] for doc in compressed_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
      "Metadata: {'id': 73, 'relevance_score': np.float32(0.9987557), 'source': 'stare_of_the_union.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "And I know you’re tired, frustrated, and exhausted. \n",
      "\n",
      "But I also know this. \n",
      "\n",
      "Because of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \n",
      "we are moving forward safely, back to more normal routines.  \n",
      "\n",
      "We’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \n",
      "\n",
      "Just a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines.\n",
      "Metadata: {'id': 55, 'relevance_score': np.float32(0.9898337), 'source': 'stare_of_the_union.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "And tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \n",
      "\n",
      "By the end of this year, the deficit will be down to less than half what it was before I took office.  \n",
      "\n",
      "The only president ever to cut the deficit by more than one trillion dollars in a single year. \n",
      "\n",
      "Lowering your costs also means demanding more competition. \n",
      "\n",
      "I’m a capitalist, but capitalism without competition isn’t capitalism. \n",
      "\n",
      "It’s exploitation—and it drives up prices.\n",
      "Metadata: {'id': 50, 'relevance_score': np.float32(0.9866672), 'source': 'stare_of_the_union.txt'}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
       " 'result': 'The president said he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson 4 days ago and that she is one of the nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
